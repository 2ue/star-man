import { Ollama } from 'ollama';
import OpenAI from 'openai';
import { AIConfig, AIAnalysisResult, ChatMessage, ChatContext } from '../types';

export class LLMService {
  private config: AIConfig;
  private ollamaClient?: Ollama;
  private openaiClient?: OpenAI;

  constructor(config: AIConfig) {
    this.config = config;

    // åˆå§‹åŒ– Ollama å®¢æˆ·ç«¯
    if (config.ollama?.baseUrl) {
      this.ollamaClient = new Ollama({
        host: config.ollama.baseUrl,
      });
    }

    // åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
    if (config.openai?.apiKey) {
      this.openaiClient = new OpenAI({
        apiKey: config.openai.apiKey,
      });
    }
  }

  /**
   * è°ƒç”¨ Ollama æ¨¡å‹
   */
  private async callOllama(prompt: string, systemPrompt?: string): Promise<string> {
    if (!this.ollamaClient) {
      throw new Error('Ollama å®¢æˆ·ç«¯æœªåˆå§‹åŒ–');
    }

    try {
      const messages: any[] = [];

      if (systemPrompt) {
        messages.push({
          role: 'system',
          content: systemPrompt,
        });
      }

      messages.push({
        role: 'user',
        content: prompt,
      });

      const response = await this.ollamaClient.chat({
        model: this.config.ollama?.model || 'llama3.2',
        messages,
      });

      return response.message.content;
    } catch (error) {
      console.error('âŒ Ollama è°ƒç”¨å¤±è´¥:', error);
      throw error;
    }
  }

  /**
   * è°ƒç”¨ OpenAI æ¨¡å‹
   */
  private async callOpenAI(prompt: string, systemPrompt?: string): Promise<string> {
    if (!this.openaiClient) {
      throw new Error('OpenAI å®¢æˆ·ç«¯æœªåˆå§‹åŒ–');
    }

    try {
      const messages: any[] = [];

      if (systemPrompt) {
        messages.push({
          role: 'system',
          content: systemPrompt,
        });
      }

      messages.push({
        role: 'user',
        content: prompt,
      });

      const response = await this.openaiClient.chat.completions.create({
        model: this.config.openai?.model || 'gpt-4o-mini',
        messages,
      });

      return response.choices[0]?.message?.content || '';
    } catch (error) {
      console.error('âŒ OpenAI è°ƒç”¨å¤±è´¥:', error);
      throw error;
    }
  }

  /**
   * è°ƒç”¨ LLMï¼ˆæ ¹æ®é…ç½®é€‰æ‹©æ¨¡å‹ï¼‰
   */
  private async callLLM(prompt: string, systemPrompt?: string): Promise<string> {
    switch (this.config.model) {
      case 'ollama':
        return await this.callOllama(prompt, systemPrompt);
      case 'openai':
        return await this.callOpenAI(prompt, systemPrompt);
      case 'gemini':
        // TODO: å®ç° Gemini
        throw new Error('Gemini æš‚æœªå®ç°');
      case 'qwen':
        // TODO: å®ç° Qwen
        throw new Error('Qwen æš‚æœªå®ç°');
      default:
        throw new Error(`ä¸æ”¯æŒçš„æ¨¡å‹: ${this.config.model}`);
    }
  }

  /**
   * æ™ºèƒ½åˆ†ç±»ä»“åº“
   */
  async categorizeRepo(repo: any): Promise<AIAnalysisResult> {
    const systemPrompt = `ä½ æ˜¯ä¸€ä¸ª GitHub ä»“åº“åˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æä»“åº“å¹¶æä¾›å‡†ç¡®çš„åˆ†ç±»å’Œæ ‡ç­¾ã€‚

åˆ†ç±»é€‰é¡¹ï¼š
- Frontend: å‰ç«¯æ¡†æ¶ã€UI ç»„ä»¶åº“ã€å‰ç«¯å·¥å…·
- Backend: åç«¯æ¡†æ¶ã€API æœåŠ¡ã€æœåŠ¡å™¨å·¥å…·
- Mobile: ç§»åŠ¨åº”ç”¨å¼€å‘ã€è·¨å¹³å°æ¡†æ¶
- DevOps: CI/CDã€å®¹å™¨åŒ–ã€éƒ¨ç½²å·¥å…·
- Data Science: æ•°æ®åˆ†æã€æœºå™¨å­¦ä¹ ã€å¯è§†åŒ–
- Tools: å¼€å‘å·¥å…·ã€CLI å·¥å…·ã€å®ç”¨ç¨‹åº
- Learning: æ•™ç¨‹ã€ç¤ºä¾‹ã€å­¦ä¹ èµ„æº
- System: ç³»ç»Ÿç¼–ç¨‹ã€åº•å±‚å·¥å…·
- Other: å…¶ä»–ç±»å‹

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ï¼š
- category: åˆ†ç±»åç§°
- tags: 3-5ä¸ªç›¸å…³æ ‡ç­¾ï¼ˆå°å†™ï¼Œç”¨è¿å­—ç¬¦åˆ†éš”ï¼‰
- confidence: ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
- reasoning: åˆ†ç±»ç†ç”±ï¼ˆç®€çŸ­è¯´æ˜ï¼‰`;

    const topics = repo.topics
      ? (typeof repo.topics === 'string' ? JSON.parse(repo.topics) : repo.topics)
      : [];

    const prompt = `åˆ†æä»¥ä¸‹ GitHub ä»“åº“ï¼š

åç§°: ${repo.name}
å®Œæ•´åç§°: ${repo.fullName || repo.full_name || ''}
æè¿°: ${repo.description || 'æ— æè¿°'}
è¯­è¨€: ${repo.language || 'æœªçŸ¥'}
Topics: ${topics.length > 0 ? topics.join(', ') : 'æ— '}
Stars: ${repo.stargazersCount || repo.stargazers_count || 0}
Forks: ${repo.forksCount || repo.forks_count || 0}

è¯·è¿”å› JSON æ ¼å¼çš„åˆ†æç»“æœã€‚`;

    try {
      const response = await this.callLLM(prompt, systemPrompt);

      // å°è¯•è§£æ JSON
      const jsonMatch = response.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        const result = JSON.parse(jsonMatch[0]);
        return {
          category: result.category || 'Other',
          tags: result.tags || [],
          confidence: result.confidence || 0.5,
          reasoning: result.reasoning || 'è‡ªåŠ¨åˆ†æ',
        };
      }

      // å¦‚æœæ— æ³•è§£æ JSONï¼Œè¿”å›é»˜è®¤å€¼
      return {
        category: 'Other',
        tags: [],
        confidence: 0.3,
        reasoning: 'æ— æ³•è§£æ AI å“åº”',
      };
    } catch (error) {
      console.error('âŒ AI åˆ†ç±»å¤±è´¥:', error);
      throw error;
    }
  }

  /**
   * ç”Ÿæˆä»“åº“æ‘˜è¦
   */
  async generateSummary(repo: any): Promise<string> {
    const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯æ–‡æ¡£ä¸“å®¶ã€‚è¯·ç”¨ç®€æ´çš„ä¸­æ–‡æ€»ç»“ GitHub ä»“åº“çš„æ ¸å¿ƒåŠŸèƒ½å’Œç‰¹ç‚¹ï¼Œä¸è¶…è¿‡ 100 å­—ã€‚`;

    const topics = repo.topics
      ? (typeof repo.topics === 'string' ? JSON.parse(repo.topics) : repo.topics)
      : [];

    const prompt = `æ€»ç»“ä»¥ä¸‹ä»“åº“ï¼š

åç§°: ${repo.name}
æè¿°: ${repo.description || 'æ— æè¿°'}
è¯­è¨€: ${repo.language || 'æœªçŸ¥'}
Topics: ${topics.length > 0 ? topics.join(', ') : 'æ— '}

è¯·ç”¨ä¸€æ®µè¯æ€»ç»“è¿™ä¸ªä»“åº“çš„æ ¸å¿ƒåŠŸèƒ½å’Œç‰¹ç‚¹ã€‚`;

    try {
      const summary = await this.callLLM(prompt, systemPrompt);
      return summary.trim();
    } catch (error) {
      console.error('âŒ ç”Ÿæˆæ‘˜è¦å¤±è´¥:', error);
      throw error;
    }
  }

  /**
   * å¯¹è¯å¼æŸ¥è¯¢ï¼ˆæµå¼å“åº”ï¼‰
   */
  async *chatStream(message: string, context: ChatContext): AsyncGenerator<string> {
    const systemPrompt = `ä½ æ˜¯ Star Manager çš„ AI åŠ©æ‰‹ã€‚ä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·ï¼š
1. æŸ¥è¯¢å’Œæœç´¢ GitHub Star ä»“åº“
2. æ¨èç›¸å…³ä»“åº“
3. åˆ†ææŠ€æœ¯æ ˆ
4. æ•´ç†å’Œåˆ†ç±»ä»“åº“

ç”¨æˆ·çš„ä»“åº“ä¿¡æ¯ä¼šåœ¨ä¸Šä¸‹æ–‡ä¸­æä¾›ã€‚è¯·ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­æ°”å›ç­”é—®é¢˜ã€‚`;

    // æ„å»ºä¸Šä¸‹æ–‡
    let contextInfo = '';
    if (context.repoContext && context.repoContext.length > 0) {
      contextInfo = '\n\nç”¨æˆ·çš„éƒ¨åˆ†ä»“åº“ä¿¡æ¯ï¼š\n';
      context.repoContext.slice(0, 10).forEach((repo: any) => {
        contextInfo += `- ${repo.name}: ${repo.description || 'æ— æè¿°'} (${repo.language || 'æœªçŸ¥è¯­è¨€'})\n`;
      });
    }

    const fullPrompt = message + contextInfo;

    // æ ¹æ®æ¨¡å‹é€‰æ‹©æµå¼å“åº”æ–¹å¼
    if (this.config.model === 'ollama' && this.ollamaClient) {
      const response = await this.ollamaClient.chat({
        model: this.config.ollama?.model || 'llama3.2',
        messages: [
          { role: 'system', content: systemPrompt },
          ...context.history.map(msg => ({
            role: msg.role,
            content: msg.content,
          })),
          { role: 'user', content: fullPrompt },
        ],
        stream: true,
      });

      for await (const part of response) {
        yield part.message.content;
      }
    } else if (this.config.model === 'openai' && this.openaiClient) {
      const stream = await this.openaiClient.chat.completions.create({
        model: this.config.openai?.model || 'gpt-4o-mini',
        messages: [
          { role: 'system', content: systemPrompt },
          ...context.history.map(msg => ({
            role: msg.role,
            content: msg.content,
          })),
          { role: 'user', content: fullPrompt },
        ],
        stream: true,
      });

      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content || '';
        if (content) {
          yield content;
        }
      }
    } else {
      throw new Error(`ä¸æ”¯æŒæµå¼å“åº”çš„æ¨¡å‹: ${this.config.model}`);
    }
  }

  /**
   * å¯¹è¯å¼æŸ¥è¯¢ï¼ˆéæµå¼ï¼‰
   */
  async chat(message: string, context: ChatContext): Promise<string> {
    const systemPrompt = `ä½ æ˜¯ Star Manager çš„ AI åŠ©æ‰‹ã€‚ä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·ï¼š
1. æŸ¥è¯¢å’Œæœç´¢ GitHub Star ä»“åº“
2. æ¨èç›¸å…³ä»“åº“
3. åˆ†ææŠ€æœ¯æ ˆ
4. æ•´ç†å’Œåˆ†ç±»ä»“åº“

ç”¨æˆ·çš„ä»“åº“ä¿¡æ¯ä¼šåœ¨ä¸Šä¸‹æ–‡ä¸­æä¾›ã€‚è¯·ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­æ°”å›ç­”é—®é¢˜ã€‚`;

    // æ„å»ºä¸Šä¸‹æ–‡
    let contextInfo = '';
    if (context.repoContext && context.repoContext.length > 0) {
      contextInfo = '\n\nç”¨æˆ·çš„éƒ¨åˆ†ä»“åº“ä¿¡æ¯ï¼š\n';
      context.repoContext.slice(0, 10).forEach((repo: any) => {
        contextInfo += `- ${repo.name}: ${repo.description || 'æ— æè¿°'} (${repo.language || 'æœªçŸ¥è¯­è¨€'})\n`;
      });
    }

    const fullPrompt = message + contextInfo;

    // æ„å»ºæ¶ˆæ¯å†å²
    const messages = [
      systemPrompt,
      ...context.history.map(msg => msg.content).join('\n'),
      fullPrompt,
    ].join('\n\n');

    return await this.callLLM(messages);
  }

  /**
   * æ‰¹é‡åˆ†æä»“åº“
   */
  async batchAnalyze(repos: any[], onProgress?: (current: number, total: number) => void): Promise<Map<number, AIAnalysisResult>> {
    const results = new Map<number, AIAnalysisResult>();
    const total = repos.length;

    console.log(`ğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ ${total} ä¸ªä»“åº“...`);

    for (let i = 0; i < repos.length; i++) {
      try {
        const result = await this.categorizeRepo(repos[i]);
        results.set(repos[i].id, result);

        if (onProgress) {
          onProgress(i + 1, total);
        }

        // é¿å…è¯·æ±‚è¿‡å¿«
        if (i < repos.length - 1) {
          await new Promise(resolve => setTimeout(resolve, 500));
        }
      } catch (error) {
        console.error(`âŒ åˆ†æä»“åº“ ${repos[i].name} å¤±è´¥:`, error);
      }
    }

    console.log(`âœ… æ‰¹é‡åˆ†æå®Œæˆ`);
    return results;
  }
}
